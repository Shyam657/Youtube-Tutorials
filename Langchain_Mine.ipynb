{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvGexO3P9WLsvI+FxzqKo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shyam657/Youtube-Tutorials/blob/main/Langchain_Mine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "tsEC7SaDxTpK",
        "outputId": "46dfa19a-b3de-4d5e-ca9e-f07f5629a726"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7e48264a95d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/38aMTXY2usU\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame('https://www.youtube.com/embed/38aMTXY2usU', width=560, height=315)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain langchain-openai langchain-core langchain_community docx2txt pypdf  langchain_chroma sentence_transformers langchain-together docx2txt pypdf unstructured together"
      ],
      "metadata": {
        "id": "GGdWGPFuxjc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#together ai api key 024254d1fcd8eee3a12258e40260e8411c8afcec22c65baca46f40aed1c60904"
      ],
      "metadata": {
        "id": "cF8aUshiyGBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = \"024254d1fcd8eee3a12258e40260e8411c8afcec22c65baca46f40aed1c60904\""
      ],
      "metadata": {
        "id": "2rqeDv81yNMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"TOGETHER_API_KEY\"):\n",
        "  os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter API key for Together AI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", model_provider=\"together\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsm3oKQAyG9I",
        "outputId": "e124b6e6-26b3-449c-f26c-c5c1bf724508"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Together AI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-course\""
      ],
      "metadata": {
        "id": "zSl0WfRdybdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_together import ChatTogether"
      ],
      "metadata": {
        "id": "0a022pUqyl4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = model.invoke(\"Tell me a joke\")\n",
        "\n",
        "llm_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KRouSWylyi",
        "outputId": "16d38d83-f4f4-453a-825a-4ae0709e81b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\" \\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 39, 'total_tokens': 94, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-63f6d25c-66f7-4749-a78d-29338e161f68-0', usage_metadata={'input_tokens': 39, 'output_tokens': 55, 'total_tokens': 94, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1X2UlXjYzhOq",
        "outputId": "4285380a-9ae2-492e-837f-4e53eeb69084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\" \\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = model | output_parser\n",
        "chain.invoke(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Xnw78vVvzhB6",
        "outputId": "6783ff4a-9dbd-4b30-9bb2-2fd51da97495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\" \\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: Optional[float] = Field(description=\"Overall rating out of 5\", default=None)  # Make rating optional with a default value\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\", default=[]) # Make pros optional with a default value\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\", default=[]) # Make cons optional with a default value\n",
        "    summary: Optional[str] = Field(description=\"Brief summary of the review\", default=None)  # Make summary optional with a default value\n",
        "\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "structured_llm = model.with_structured_output(MobileReview)\n",
        "output = structured_llm.invoke(review_text)\n",
        "\n",
        "# Check if the output is not None before accessing attributes\n",
        "if output:\n",
        "    print(output.summary)\n",
        "else:\n",
        "    print(\"The model was unable to extract information from the review text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw08ZH1Dzn-7",
        "outputId": "6299c68a-78f4-4b86-8a9a-6497748b9315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great phone, but a few annoying quirks keep it from being perfect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.pros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFRr9esZ0Abw",
        "outputId": "98a5c1a3-8a66-403c-929c-f1e11feffa80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['screen is gorgeous', 'colors pop', \"camera's insane\", \"battery life's solid\"]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.rating"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvGLHuO0BF7",
        "outputId": "c09731a2-ead3-47e0-a9b9-d4a5e31d1738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.phone_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SkKxBdz-0qLZ",
        "outputId": "53da16a1-d300-46df-f77d-e5b334d7b486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Galaxy S21'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "prompt.invoke({\"topic\": \"programming\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddebSUz60v3w",
        "outputId": "d1d57b92-2066-48b3-91f4-ec3f731d5c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Tell me a short joke about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser\n",
        "chain.invoke({\"topic\": \"programer\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5oaQaJ9R00IQ",
        "outputId": "92fce590-d985-4e8f-f7b0-579c64399222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_together import ChatTogether\n",
        "# Define the prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")"
      ],
      "metadata": {
        "id": "UsLUrAbV02jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying chat models with Together AI\n",
        "\n",
        "from langchain_together import ChatTogether\n",
        "\n",
        "# choose from our 50+ models here: https://docs.together.ai/docs/inference-models\n",
        "chat = ChatTogether(\n",
        "    # together_api_key=\"YOUR_API_KEY\",\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        ")\n",
        "\n",
        "\n",
        "# Compose the chain\n",
        "chain = prompt | chat | output_parser\n",
        "\n",
        "# Use the chain\n",
        "result = chain.invoke({\"topic\": \"programming\"})\n",
        "print(result)\n",
        "\n",
        "\n",
        "\n",
        "# stream the response back from the model\n",
        "#for m in chat.stream(\"Tell me fun things to do in NYC\"):\n",
        " #   print(m.content, end=\"\", flush=True)\n",
        "\n",
        "# if you don't want to do streaming, you can use the invoke method\n",
        "# chat.invoke(\"Tell me fun things to do in NYC\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaKZm-0-02ap",
        "outputId": "4cba2e22-f90e-4a52-c150-d907c0bf1936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compose the chain\n",
        "chain = prompt | chat | output_parser\n",
        "\n",
        "# Use the chain\n",
        "result = chain.invoke({\"topic\": \"programming\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhAqTi4g3Y24",
        "outputId": "098d364a-ee3d-4783-df5d-f389401783f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"topic\": \"programming\"\n",
        "    }\n",
        ")\n",
        "prompt_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BitK83ll3cC3",
        "outputId": "b22cedf6-2129-4efe-c567-11663a789d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"topic\": \"programming\"\n",
        "    }\n",
        ")\n",
        "prompt_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PElkgE13ium",
        "outputId": "00f120cf-720d-474f-9120-99e21467f30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(prompt_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-Sd2Z9V3l7M",
        "outputId": "97b20f0d-41a0-4eac-a967-eb48a2d8293c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Programming, it's a world of code, bugs, and endless coffee cups. But don't worry, I'm here to help you navigate it.\\n\\nProgramming is the process of designing, writing, testing, and maintaining the source code of computer programs. It's like writing a recipe for your computer to follow, but instead of ingredients, you use programming languages like Python, Java, or C++.\\n\\nImagine you're trying to make a peanut butter and jelly sandwich. You need to tell the computer exactly what to do, step by step:\\n\\n1. Get two slices of bread.\\n2. Spread peanut butter on one slice.\\n3. Spread jelly on the other slice.\\n4. Put the two slices together.\\n\\nIn programming, you would write code that says something like:\\n\\n`bread = get_bread()`\\n`peanut_butter = spread_peanut_butter(bread)`\\n`jelly = spread_jelly(bread)`\\n`sandwich = combine_slices(peanut_butter, jelly)`\\n\\nBut, just like in real life, things don't always go as planned. You might get a bug in your code, like a typo or a logical error. That's when the debugging process begins, and you get to play detective to figure out what went wrong.\\n\\nHere's a joke to lighten the mood:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs.\\n\\nI hope that helps you understand programming a little better. Do you have any specific questions or topics you'd like to explore further?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 302, 'prompt_tokens': 48, 'total_tokens': 350, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3d57b933-2919-49b5-848e-a46922995589-0', usage_metadata={'input_tokens': 48, 'output_tokens': 302, 'total_tokens': 350, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir docs"
      ],
      "metadata": {
        "id": "80-FJfeE30s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0accfe25-2766-4087-9e47-925b9de752fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘docs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "docx_loader = Docx2txtLoader(\"/content/docs/GreenGrow Innovations_ Company History.docx\")\n",
        "documents = docx_loader.load()\n",
        "\n",
        "print(len(documents))\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVb_GfN130jb",
        "outputId": "771b1243-37f5-4a69-e7d1-11824b20bd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Split the documents into 2 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auW2Py8P3-4x",
        "outputId": "cfce2f09-99c3-4d74-ea3e-b0e60f4d5af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.\\n\\n\\n\\nThe company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ERTExOp4Kp1",
        "outputId": "187799cb-5ab4-4730-a2b4-3c04457a7f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"The company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO_flAe14MCf",
        "outputId": "513984aa-787b-464c-dadb-0f63b0ca310d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "I86lba4b4L-u",
        "outputId": "e2ee0613-b7fe-4d46-e08a-dfe581bd0c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNfSMJlP4L7V",
        "outputId": "9c4ea679-1086-4801-ad93-b75c5707ae54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Function to load documents from a folder\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"/content/docs\"\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWBjP2cU4L3f",
        "outputId": "4813afc8-ed8b-49f5-d567-ffb069c5a622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5 documents from the folder.\n",
            "Split the documents into 8 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztaE1gD64L1J",
        "outputId": "b42d441a-18a4-42f6-b376-89eb8fbad047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from together import Together\n",
        "\n",
        "client = Together()\n",
        "\n",
        "# Extract text content from Document objects and create a list of strings\n",
        "input_texts = [doc.page_content for doc in splits]\n",
        "\n",
        "# Pass the list of strings to the embeddings.create method\n",
        "response = client.embeddings.create(\n",
        "    model=\"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
        "    input=input_texts\n",
        ")\n"
      ],
      "metadata": {
        "id": "sjVbWclE4Ly9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.data"
      ],
      "metadata": {
        "id": "UXj-z4xM4Lwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Created embeddings for {len(response.data)} document chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMoy0IOu7QU8",
        "outputId": "ad7d235c-c19a-450b-b2c6-5f518a0f3005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embeddings for 8 document chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
        "document_embeddings[0]"
      ],
      "metadata": {
        "id": "1P36193K-JqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_chroma -q"
      ],
      "metadata": {
        "id": "4ZZ-_OYK7ctV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from together import Together"
      ],
      "metadata": {
        "id": "9lJck2lp-dDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = Together()\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ],
      "metadata": {
        "id": "ZT19nQ7R-cqr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "421cbecb-b235-4b5a-f0f5-53d627a2cda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Together' object has no attribute 'embed_documents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-6f6c1bfa5b0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTogether\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcollection_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"my_collection\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#db.persist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m   1240\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             ):\n\u001b[0;32m-> 1192\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m   1193\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;31m# fill metadatas with empty dicts if somebody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Together' object has no attribute 'embed_documents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-together"
      ],
      "metadata": {
        "id": "vAkRqoSstSGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_together import TogetherEmbeddings\n",
        "\n",
        "embeddings = TogetherEmbeddings(\n",
        "    model=\"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
        ")"
      ],
      "metadata": {
        "id": "uwi9s90dtpK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnAfSimdu3xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_together import TogetherEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "'''\n",
        "# Step 1: Create or load your documents\n",
        "raw_text = \"This is a sample document. It contains multiple sentences. We want to split it into smaller chunks.\"\n",
        "doc = Document(page_content=raw_text, metadata={\"source\": \"example\"})\n",
        "\n",
        "# Step 2: Split the documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "splits = text_splitter.split_documents([doc])\n",
        "'''\n",
        "# Step 3: Create the embedding function using Together API\n",
        "embedding_function = TogetherEmbeddings(\n",
        "    model=\"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
        "    # api_key=\"your-together-api-key\"  # Uncomment and add your API key if not set as an environment variable\n",
        ")\n",
        "\n",
        "# Step 4: Create the vector store\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    collection_name=collection_name,\n",
        "    documents=splits,\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HXa0sjpvQqx",
        "outputId": "a5c1658f-a5bc-47f6-909e-f0efa40af94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Perform similarity search\n",
        "\n",
        "query = \"When was GreenGrow Innovations founded?\"\n",
        "search_results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nas2BVP_s7a_",
        "outputId": "f3cc684f-2b54-4e45-c469-b7283e4d41c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 2 most relevant chunks for the query: 'When was GreenGrow Innovations founded?'\n",
            "\n",
            "Result 1:\n",
            "Source: /content/docs/GreenGrow Innovations_ Company History.docx\n",
            "Content: By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research\n",
            "\n",
            "Result 2:\n",
            "Source: /content/docs/GreenGrow Innovations_ Company History.docx\n",
            "Content: By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever.invoke(\"When was GreenGrow Innovations founded?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZp63VFlyObj",
        "outputId": "931a64ad-25cd-4667-c318-c46c11bdfda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ecc87323-d0d1-42f3-883e-8d00fff41686', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research'),\n",
              " Document(id='414d69e4-d43c-43c9-bd34-681d43e60604', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "29niSa00yOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(\"When was GreenGrow Innovations founded?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoyscRoV0Ylb",
        "outputId": "4ea06832-5fa4-434a-aa40-967c6dec046f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"Answer the question based only on the following context:\\n[Document(id='ecc87323-d0d1-42f3-883e-8d00fff41686', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research'), Document(id='414d69e4-d43c-43c9-bd34-681d43e60604', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research')]\\n\\nQuestion: When was GreenGrow Innovations founded?\\n\\nAnswer: \", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "dmo-f32b0eWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(\"When was GreenGrow Innovations founded?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3K_tXjm0fVp",
        "outputId": "1d45a0ef-43be-4751-99fe-efde7d123364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the following context:\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research\\n\\nQuestion: When was GreenGrow Innovations founded?\\n\\nAnswer: ', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = \"When was GreenGrow Innovations founded?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAMCpxqg0fS1",
        "outputId": "11c11a62-3144-4323-be0b-9385a6a02ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided context does not mention the founding year of GreenGrow. It only mentions that by 2015, GreenGrow had outgrown its garage origins, implying that it was founded before 2015, but the exact year is not specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ],
      "metadata": {
        "id": "rcfdM8GI0fQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON_i7sIe0fOs",
        "outputId": "a9e35485-ab89-4b37-a9a6-f6fdbec64211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The provided context does not mention the founding year of GreenGrow. It only mentions that by 2015, GreenGrow had outgrown its garage origins, implying that it was founded before 2015, but the exact year is not specified.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "contextualize_chain = contextualize_q_prompt | model | StrOutputParser()\n",
        "contextualize_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jCoWFcB-0fIG",
        "outputId": "56a61362-f3ba-4391-f47d-642bbe5f5ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No information is provided about the headquarters of GreenGrow Innovations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    model, retriever, contextualize_q_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSWr4nqN0s7f",
        "outputId": "4356d86b-f273-4f35-edd2-3112929975c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='3ad5f438-2caf-4699-a5e8-c4d080e8ced3', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='Today, GreenGrow Innovations employs over 200 people and has expanded its operations to include'),\n",
              " Document(id='92d223ca-b292-431b-84f0-65d4566dad64', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='Today, GreenGrow Innovations employs over 200 people and has expanded its operations to include')]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever.invoke(\"Where it is headquartered?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q64sH_Bt0s5p",
        "outputId": "84129174-a0cc-46b2-fa82-5ed34a377c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='414d69e4-d43c-43c9-bd34-681d43e60604', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research'),\n",
              " Document(id='ecc87323-d0d1-42f3-883e-8d00fff41686', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='By 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research')]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    #  (\"system\", \"Tell me joke on Programming\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "qwKDBfTd0syu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\":chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC362WAW1SH6",
        "outputId": "5c86c4de-630d-41b0-dc9e-d673acd20f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Where it is headquartered?',\n",
              " 'chat_history': [HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The provided context does not mention the founding year of GreenGrow. It only mentions that by 2015, GreenGrow had outgrown its garage origins, implying that it was founded before 2015, but the exact year is not specified.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(id='3ad5f438-2caf-4699-a5e8-c4d080e8ced3', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='Today, GreenGrow Innovations employs over 200 people and has expanded its operations to include'),\n",
              "  Document(id='92d223ca-b292-431b-84f0-65d4566dad64', metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='Today, GreenGrow Innovations employs over 200 people and has expanded its operations to include')],\n",
              " 'answer': \"The provided context does not mention the location of GreenGrow Innovations' headquarters.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ],
      "metadata": {
        "id": "qO5SYE7h1XN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"When was GreenGrow Innovations founded?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ikNoR41XKM",
        "outputId": "68ae7764-43c4-4877-e2c1-dab15766750e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Human: When was GreenGrow Innovations founded?\n",
            "AI: Unfortunately, the provided context does not mention the exact founding date of GreenGrow. However, it does mention that by 2015, GreenGrow had outgrown its garage origins, suggesting that it was founded sometime before 2015.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question2 = \"Where it is headquartered?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBx7QRF41XFg",
        "outputId": "c047aa32-5270-4080-f53d-59ac7679bc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'human', 'content': 'When was GreenGrow Innovations founded?'}, {'role': 'ai', 'content': 'Unfortunately, the provided context does not mention the exact founding date of GreenGrow. However, it does mention that by 2015, GreenGrow had outgrown its garage origins, suggesting that it was founded sometime before 2015.'}]\n",
            "Human: Where it is headquartered?\n",
            "AI: Unfortunately, the provided context does not mention the location of GreenGrow's headquarters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is GreenGrow\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBYuYKJR2n_V",
        "outputId": "320447cd-6d53-460d-cfb1-27148ba10651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Human: What is GreenGrow\n",
            "AI: I don't have any information about \"GreenGrow\" in the context you provided. Could you please provide more context or details about what GreenGrow is or what it relates to? This will help me better understand your question and provide a more accurate answer.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGrjU8JU2n8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7z8YNnH2n6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jR-oWskA2n4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}